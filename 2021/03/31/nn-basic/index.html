<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="zh-CN">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="最近复习春招，整理了一下之前写的CS231n里的作业的内容方便复习😀">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络常用运算&amp;优化器">
<meta property="og:url" content="http://blog.4f5da2.com/2021/03/31/nn-basic/index.html">
<meta property="og:site_name" content="4F5DA2&#39;s BLOG">
<meta property="og:description" content="最近复习春招，整理了一下之前写的CS231n里的作业的内容方便复习😀">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-04-30T07:29:38.665Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="神经网络常用运算&amp;优化器">
<meta name="twitter:description" content="最近复习春招，整理了一下之前写的CS231n里的作业的内容方便复习😀">






  <link rel="canonical" href="http://blog.4f5da2.com/2021/03/31/nn-basic/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>神经网络常用运算&优化器 | 4F5DA2's BLOG</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">4F5DA2's BLOG</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
      
        <p class="site-subtitle">Quietly Brilliant</p>
      
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://blog.4f5da2.com/2021/03/31/nn-basic/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="4F5DA2">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="4F5DA2's BLOG">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">神经网络常用运算&优化器

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              

              
                
              

              <time title="创建时间：2021-04-01 00:44:38" itemprop="dateCreated datePublished" datetime="2021-04-01T00:44:38+08:00">2021-04-01</time>
            

            
          </span>

          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <a href="/2021/03/31/nn-basic/#comments" itemprop="discussionUrl">
                  <span class="post-meta-item-text">评论数：</span> <span class="post-comments-count valine-comment-count" data-xid="/2021/03/31/nn-basic/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>最近复习春招，整理了一下之前写的CS231n里的作业的内容方便复习😀</p>
<a id="more"></a>
<h2 id="常用运算及其求导"><a href="#常用运算及其求导" class="headerlink" title="常用运算及其求导"></a>常用运算及其求导</h2><h3 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    scores = X @ W</span><br><span class="line">    score_exp_sum = np.sum(np.exp(scores), axis=<span class="number">1</span>)</span><br><span class="line">    loss += np.sum(-scores[np.arange(num_train), y] + np.log(score_exp_sum))</span><br><span class="line">    d_scores = np.zeros_like(scores)</span><br><span class="line">    d_scores[np.arange(num_train), y] -= <span class="number">1</span></span><br><span class="line">    d_scores += <span class="number">1</span> / score_exp_sum[...,<span class="keyword">None</span>] * np.exp(scores)</span><br><span class="line">    dW = X.T @ d_scores</span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line">    dW += <span class="number">2</span> * reg * W</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h3 id="svm-loss"><a href="#svm-loss" class="headerlink" title="svm loss"></a>svm loss</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss</span><span class="params">(x, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the loss and gradient using for multiclass SVM classification.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth</span></span><br><span class="line"><span class="string">      class for the ith input.</span></span><br><span class="line"><span class="string">    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and</span></span><br><span class="line"><span class="string">      0 &lt;= y[i] &lt; C</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Scalar giving the loss</span></span><br><span class="line"><span class="string">    - dx: Gradient of the loss with respect to x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = x.shape[<span class="number">0</span>]</span><br><span class="line">    correct_class_scores = x[np.arange(N), y]</span><br><span class="line">    margins = np.maximum(<span class="number">0</span>, x - correct_class_scores[:, np.newaxis] + <span class="number">1.0</span>)</span><br><span class="line">    margins[np.arange(N), y] = <span class="number">0</span></span><br><span class="line">    loss = np.sum(margins) / N</span><br><span class="line">    num_pos = np.sum(margins &gt; <span class="number">0</span>, axis=<span class="number">1</span>)</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line">    dx[margins &gt; <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    dx[np.arange(N), y] -= num_pos</span><br><span class="line">    dx /= N</span><br><span class="line">    <span class="keyword">return</span> loss, dx</span><br></pre></td></tr></table></figure>
<h3 id="fully-connected"><a href="#fully-connected" class="headerlink" title="fully connected"></a>fully connected</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for an affine (fully-connected) layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span></span><br><span class="line"><span class="string">    examples, where each example x[i] has shape (d_1, ..., d_k). We will</span></span><br><span class="line"><span class="string">    reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span></span><br><span class="line"><span class="string">    then transform it to an output vector of dimension M.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span></span><br><span class="line"><span class="string">    - w: A numpy array of weights, of shape (D, M)</span></span><br><span class="line"><span class="string">    - b: A numpy array of biases, of shape (M,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: output, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: (x, w, b)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    x_reshaped = x.reshape(x.shape[<span class="number">0</span>], <span class="number">-1</span>)</span><br><span class="line">    out = x_reshaped @ w + b</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    cache = (x, w, b)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for an affine layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivative, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: Tuple of:</span></span><br><span class="line"><span class="string">      - x: Input data, of shape (N, d_1, ... d_k)</span></span><br><span class="line"><span class="string">      - w: Weights, of shape (D, M)</span></span><br><span class="line"><span class="string">      - b: Biases, of shape (M,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w, of shape (D, M)</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b, of shape (M,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x, w, b = cache</span><br><span class="line">    dx, dw, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    n, m = dout.shape</span><br><span class="line">    dx = dout @ w.T</span><br><span class="line">    dw = x.reshape(n, <span class="number">-1</span>).T @ dout</span><br><span class="line">    db = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    dx = dx.reshape(x.shape)</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure>
<h3 id="batch-norm"><a href="#batch-norm" class="headerlink" title="batch norm"></a>batch norm</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">"mode"</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">"eps"</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">"momentum"</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">"running_mean"</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">"running_var"</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"train"</span>:</span><br><span class="line">        <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">        sample_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        sample_var = np.var(x, axis=<span class="number">0</span>)</span><br><span class="line">        x_hat = (x - sample_mean) / np.sqrt(sample_var + eps)</span><br><span class="line">        out = gamma * x_hat + beta</span><br><span class="line">        cache = (x_hat, x, sample_mean, sample_var, gamma, beta, eps)</span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * sample_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * sample_var</span><br><span class="line">        <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">"test"</span>:</span><br><span class="line">        <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">        x_hat = (x - running_mean) / np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma * x_hat + beta</span><br><span class="line">        <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">"running_mean"</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">"running_var"</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    <span class="comment"># (https://arxiv.org/abs/1502.03167) </span></span><br><span class="line"></span><br><span class="line">    x_hat, x, sample_mean, sample_var, gamma, beta, eps = cache</span><br><span class="line">    n = x.shape[<span class="number">0</span>]</span><br><span class="line">    d_x_hat = dout * gamma</span><br><span class="line">    d_x1 = d_x_hat / np.sqrt(sample_var + eps)</span><br><span class="line">    d_sample_mean1 = np.sum(-d_x_hat / np.sqrt(sample_var + eps), axis=<span class="number">0</span>)</span><br><span class="line">    d_sample_var = np.sum((x - sample_mean) * <span class="number">-0.5</span> * np.power((sample_var + eps), <span class="number">-3</span>/<span class="number">2</span>) * d_x_hat, axis=<span class="number">0</span>)</span><br><span class="line">    d_x2 = <span class="number">2</span> * (x - sample_mean) * d_sample_var / n</span><br><span class="line">    d_sample_mean2 = np.sum(<span class="number">-2</span> * (x - sample_mean) * d_sample_var, axis=<span class="number">0</span>) / n</span><br><span class="line">    d_sample_mean = d_sample_mean1 + d_sample_mean2</span><br><span class="line">    d_x3 = d_sample_mean / n</span><br><span class="line"></span><br><span class="line">    dx = d_x1 + d_x2 + d_x3</span><br><span class="line">    dgamma = np.sum(dout * x_hat, axis=<span class="number">0</span>)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_forward</span><span class="params">(x, dropout_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the forward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of any shape</span></span><br><span class="line"><span class="string">    - dropout_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - p: Dropout parameter. We keep each neuron output with probability p.</span></span><br><span class="line"><span class="string">      - mode: 'test' or 'train'. If the mode is train, then perform dropout;</span></span><br><span class="line"><span class="string">        if the mode is test, then just return the input.</span></span><br><span class="line"><span class="string">      - seed: Seed for the random number generator. Passing seed makes this</span></span><br><span class="line"><span class="string">        function deterministic, which is needed for gradient checking but not</span></span><br><span class="line"><span class="string">        in real networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">    - out: Array of the same shape as x.</span></span><br><span class="line"><span class="string">    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout</span></span><br><span class="line"><span class="string">      mask that was used to multiply the input; in test mode, mask is None.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    NOTE: Please implement **inverted** dropout, not the vanilla version of dropout.</span></span><br><span class="line"><span class="string">    See http://cs231n.github.io/neural-networks-2/#reg for more details.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    NOTE 2: Keep in mind that p is the probability of **keep** a neuron</span></span><br><span class="line"><span class="string">    output; this might be contrary to some sources, where it is referred to</span></span><br><span class="line"><span class="string">    as the probability of dropping a neuron output.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    p, mode = dropout_param[<span class="string">"p"</span>], dropout_param[<span class="string">"mode"</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">"seed"</span> <span class="keyword">in</span> dropout_param:</span><br><span class="line">        np.random.seed(dropout_param[<span class="string">"seed"</span>])</span><br><span class="line"></span><br><span class="line">    mask = <span class="keyword">None</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"train"</span>:</span><br><span class="line">        <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">        mask = np.random.rand(*x.shape) &lt; p</span><br><span class="line">        out = x * mask / p</span><br><span class="line">        <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">"test"</span>:</span><br><span class="line">        <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">        out = x</span><br><span class="line">        <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line"></span><br><span class="line">    cache = (dropout_param, mask)</span><br><span class="line">    out = out.astype(x.dtype, copy=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Perform the backward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></span><br><span class="line"><span class="string">    - cache: (dropout_param, mask) from dropout_forward.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dropout_param, mask = cache</span><br><span class="line">    mode = dropout_param[<span class="string">"mode"</span>]</span><br><span class="line"></span><br><span class="line">    dx = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">"train"</span>:</span><br><span class="line">        <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">        dx = dout * mask / dropout_param[<span class="string">"p"</span>]</span><br><span class="line">        <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">"test"</span>:</span><br><span class="line">        dx = dout</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<h3 id="conv"><a href="#conv" class="headerlink" title="conv"></a>conv</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_forward_naive</span><span class="params">(x, w, b, conv_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input consists of N data points, each with C channels, height H and</span></span><br><span class="line"><span class="string">    width W. We convolve each input with F different filters, where each filter</span></span><br><span class="line"><span class="string">    spans all C channels and has height HH and width WW.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Input data of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - w: Filter weights of shape (F, C, HH, WW)</span></span><br><span class="line"><span class="string">    - b: Biases, of shape (F,)</span></span><br><span class="line"><span class="string">    - conv_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - 'stride': The number of pixels between adjacent receptive fields in the</span></span><br><span class="line"><span class="string">        horizontal and vertical directions.</span></span><br><span class="line"><span class="string">      - 'pad': The number of pixels that will be used to zero-pad the input. </span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)</span></span><br><span class="line"><span class="string">    along the height and width axes of the input. Be careful not to modfiy the original</span></span><br><span class="line"><span class="string">    input x directly.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, F, H', W') where H' and W' are given by</span></span><br><span class="line"><span class="string">      H' = 1 + (H + 2 * pad - HH) / stride</span></span><br><span class="line"><span class="string">      W' = 1 + (W + 2 * pad - WW) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, w, b, conv_param)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    n, c, xh, xw = x.shape</span><br><span class="line">    f, _, hh, ww = w.shape</span><br><span class="line">    p = conv_param[<span class="string">"pad"</span>]</span><br><span class="line">    s = conv_param[<span class="string">"stride"</span>]</span><br><span class="line">    h_ = int(<span class="number">1</span> + (xh + <span class="number">2</span> * p - hh) / s)</span><br><span class="line">    w_ = int(<span class="number">1</span> + (xw + <span class="number">2</span> * p - ww) / s)</span><br><span class="line">    out = np.zeros((n, f, h_, w_))</span><br><span class="line">    x_paded = np.pad(x, ((<span class="number">0</span>,<span class="number">0</span>),(<span class="number">0</span>,<span class="number">0</span>),(p,p),(p,p)), mode=<span class="string">"constant"</span>, constant_values=<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(h_):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(w_):</span><br><span class="line">        ii = i * s</span><br><span class="line">        jj = j * s</span><br><span class="line">        temp = x_paded[:,<span class="keyword">None</span>,:,ii:ii+hh,jj:jj+ww] * w</span><br><span class="line">        temp = np.sum(temp, axis=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)) + b</span><br><span class="line">        out[:,:,i,j] = temp</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    cache = (x, w, b, conv_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a convolutional layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives.</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dw, db = <span class="keyword">None</span>, <span class="keyword">None</span>, <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    x, w, b, conv_param = cache</span><br><span class="line"></span><br><span class="line">    n, c, xh, xw = x.shape</span><br><span class="line">    f, _, hh, ww = w.shape</span><br><span class="line">    p = conv_param[<span class="string">"pad"</span>]</span><br><span class="line">    s = conv_param[<span class="string">"stride"</span>]</span><br><span class="line">    h_ = int(<span class="number">1</span> + (xh + <span class="number">2</span> * p - hh) / s)</span><br><span class="line">    w_ = int(<span class="number">1</span> + (xw + <span class="number">2</span> * p - ww) / s)</span><br><span class="line">    x_paded = np.pad(x, ((<span class="number">0</span>,<span class="number">0</span>),(<span class="number">0</span>,<span class="number">0</span>),(p,p),(p,p)), mode=<span class="string">"constant"</span>, constant_values=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    dx_paded = np.zeros((x.shape[<span class="number">0</span>], x.shape[<span class="number">1</span>], x.shape[<span class="number">2</span>] + <span class="number">2</span> * p, x.shape[<span class="number">3</span>] + <span class="number">2</span> * p))</span><br><span class="line">    dw = np.zeros_like(w)</span><br><span class="line">    db = np.zeros_like(b)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(h_):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(w_):</span><br><span class="line">        ii = i * s</span><br><span class="line">        jj = j * s</span><br><span class="line">        d_temp = dout[:,:,i,j]</span><br><span class="line">        db += np.sum(d_temp, axis=<span class="number">0</span>)</span><br><span class="line">        dw += np.sum(x_paded[:,<span class="keyword">None</span>,:,ii:ii+hh,jj:jj+ww] * d_temp[:,:,<span class="keyword">None</span>,<span class="keyword">None</span>,<span class="keyword">None</span>], axis=<span class="number">0</span>)</span><br><span class="line">        dx_paded[:,:, ii:ii+hh, jj:jj+ww] += np.sum(w * d_temp[:,:,<span class="keyword">None</span>,<span class="keyword">None</span>,<span class="keyword">None</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">if</span> p &gt; <span class="number">0</span>:</span><br><span class="line">      dx = dx_paded[:,:,p:-p,p:-p]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      dx = dx_paded</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure>
<h3 id="max-pool"><a href="#max-pool" class="headerlink" title="max pool"></a>max pool</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_forward_naive</span><span class="params">(x, pool_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the forward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of shape (N, C, H, W)</span></span><br><span class="line"><span class="string">    - pool_param: dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - 'pool_height': The height of each pooling region</span></span><br><span class="line"><span class="string">      - 'pool_width': The width of each pooling region</span></span><br><span class="line"><span class="string">      - 'stride': The distance between adjacent pooling regions</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    No padding is necessary here. Output size is given by </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output data, of shape (N, C, H', W') where H' and W' are given by</span></span><br><span class="line"><span class="string">      H' = 1 + (H - pool_height) / stride</span></span><br><span class="line"><span class="string">      W' = 1 + (W - pool_width) / stride</span></span><br><span class="line"><span class="string">    - cache: (x, pool_param)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    ph = pool_param[<span class="string">"pool_height"</span>]</span><br><span class="line">    pw = pool_param[<span class="string">"pool_width"</span>]</span><br><span class="line">    s = pool_param[<span class="string">"stride"</span>]</span><br><span class="line">    n, c, xh, xw = x.shape</span><br><span class="line">    h_ = int(<span class="number">1</span> + (xh - ph) / s)</span><br><span class="line">    w_ = int(<span class="number">1</span> + (xw - pw) / s)</span><br><span class="line">    out = np.zeros((n, c, h_, w_))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(h_):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(w_):</span><br><span class="line">        ii = i * s</span><br><span class="line">        jj = j * s</span><br><span class="line">        out[:, :, i, j] = np.max(x[:, :, ii:ii+ph, jj:jj+pw], axis=(<span class="number">-2</span>, <span class="number">-1</span>))</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    cache = (x, pool_param)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_backward_naive</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A naive implementation of the backward pass for a max-pooling layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives</span></span><br><span class="line"><span class="string">    - cache: A tuple of (x, pool_param) as in the forward pass.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx = <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    x, pool_param = cache</span><br><span class="line">    ph = pool_param[<span class="string">"pool_height"</span>]</span><br><span class="line">    pw = pool_param[<span class="string">"pool_width"</span>]</span><br><span class="line">    s = pool_param[<span class="string">"stride"</span>]</span><br><span class="line">    n, c, xh, xw = x.shape</span><br><span class="line">    h_ = int(<span class="number">1</span> + (xh - ph) / s)</span><br><span class="line">    w_ = int(<span class="number">1</span> + (xw - pw) / s)</span><br><span class="line">    dx = np.zeros_like(x)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(h_):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(w_):</span><br><span class="line">        ii = i * s</span><br><span class="line">        jj = j * s</span><br><span class="line">        temp = x[:, :, ii:ii+ph, jj:jj+pw]</span><br><span class="line">        max_val = np.max(temp, axis=(<span class="number">-2</span>,<span class="number">-1</span>))</span><br><span class="line">        d_temp = dx[:, :, ii:ii+ph, jj:jj+pw]</span><br><span class="line">        d_temp[max_val[..., <span class="keyword">None</span>, <span class="keyword">None</span>] == temp] = dout[:,:,i,j].reshape(<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<h2 id="常用优化器"><a href="#常用优化器" class="headerlink" title="常用优化器"></a>常用优化器</h2><h3 id="sgd"><a href="#sgd" class="headerlink" title="sgd"></a>sgd</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs vanilla stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">"learning_rate"</span>, <span class="number">1e-2</span>)</span><br><span class="line"></span><br><span class="line">    w -= config[<span class="string">"learning_rate"</span>] * dw</span><br><span class="line">    <span class="keyword">return</span> w, config</span><br></pre></td></tr></table></figure>
<h3 id="sgd-w-momentum"><a href="#sgd-w-momentum" class="headerlink" title="sgd w/ momentum"></a>sgd w/ momentum</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs stochastic gradient descent with momentum.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    - momentum: Scalar between 0 and 1 giving the momentum value.</span></span><br><span class="line"><span class="string">      Setting momentum = 0 reduces to sgd.</span></span><br><span class="line"><span class="string">    - velocity: A numpy array of the same shape as w and dw used to store a</span></span><br><span class="line"><span class="string">      moving average of the gradients.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">"learning_rate"</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">"momentum"</span>, <span class="number">0.9</span>)</span><br><span class="line">    v = config.get(<span class="string">"velocity"</span>, np.zeros_like(w))</span><br><span class="line"></span><br><span class="line">    next_w = <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    v = config[<span class="string">"momentum"</span>] * v  - config[<span class="string">"learning_rate"</span>] * dw</span><br><span class="line">    next_w = w + v</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    config[<span class="string">"velocity"</span>] = v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br></pre></td></tr></table></figure>
<h3 id="rmsprop"><a href="#rmsprop" class="headerlink" title="rmsprop"></a>rmsprop</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the RMSProp update rule, which uses a moving average of squared</span></span><br><span class="line"><span class="string">    gradient values to set adaptive per-parameter learning rates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared</span></span><br><span class="line"><span class="string">      gradient cache.</span></span><br><span class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></span><br><span class="line"><span class="string">    - cache: Moving average of second moments of gradients.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">"learning_rate"</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">"decay_rate"</span>, <span class="number">0.99</span>)</span><br><span class="line">    config.setdefault(<span class="string">"epsilon"</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">"cache"</span>, np.zeros_like(w))</span><br><span class="line"></span><br><span class="line">    next_w = <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    cache = config[<span class="string">"decay_rate"</span>] * config[<span class="string">"cache"</span>] + (<span class="number">1</span> - config[<span class="string">"decay_rate"</span>]) * dw ** <span class="number">2</span></span><br><span class="line">    next_w = w - config[<span class="string">"learning_rate"</span>] * dw/ (np.sqrt(cache) + config[<span class="string">"epsilon"</span>])</span><br><span class="line">    config[<span class="string">"cache"</span>] = cache</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br></pre></td></tr></table></figure>
<h3 id="adam"><a href="#adam" class="headerlink" title="adam"></a>adam</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the Adam update rule, which incorporates moving averages of both the</span></span><br><span class="line"><span class="string">    gradient and its square and a bias correction term.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    - beta1: Decay rate for moving average of first moment of gradient.</span></span><br><span class="line"><span class="string">    - beta2: Decay rate for moving average of second moment of gradient.</span></span><br><span class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></span><br><span class="line"><span class="string">    - m: Moving average of gradient.</span></span><br><span class="line"><span class="string">    - v: Moving average of squared gradient.</span></span><br><span class="line"><span class="string">    - t: Iteration number.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">"learning_rate"</span>, <span class="number">1e-3</span>)</span><br><span class="line">    config.setdefault(<span class="string">"beta1"</span>, <span class="number">0.9</span>)</span><br><span class="line">    config.setdefault(<span class="string">"beta2"</span>, <span class="number">0.999</span>)</span><br><span class="line">    config.setdefault(<span class="string">"epsilon"</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">"m"</span>, np.zeros_like(w))</span><br><span class="line">    config.setdefault(<span class="string">"v"</span>, np.zeros_like(w))</span><br><span class="line">    config.setdefault(<span class="string">"t"</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    next_w = <span class="keyword">None</span></span><br><span class="line">    <span class="comment"># *****START OF CORE CODE*****</span></span><br><span class="line">    config[<span class="string">"t"</span>] += <span class="number">1</span></span><br><span class="line">    m = config[<span class="string">"beta1"</span>] * config[<span class="string">"m"</span>] + (<span class="number">1</span> - config[<span class="string">"beta1"</span>]) * dw</span><br><span class="line">    mt = m / (<span class="number">1</span> - config[<span class="string">"beta1"</span>] ** config[<span class="string">"t"</span>])</span><br><span class="line">    v = config[<span class="string">"beta2"</span>] * config[<span class="string">"v"</span>] + (<span class="number">1</span> - config[<span class="string">"beta2"</span>]) * (dw ** <span class="number">2</span>)</span><br><span class="line">    vt = v / (<span class="number">1</span> - config[<span class="string">"beta2"</span>] ** config[<span class="string">"t"</span>])</span><br><span class="line">    next_w = w - config[<span class="string">"learning_rate"</span>] * mt / (np.sqrt(vt) + config[<span class="string">"epsilon"</span>])</span><br><span class="line">    config[<span class="string">"m"</span>] = m</span><br><span class="line">    config[<span class="string">"v"</span>] = v</span><br><span class="line">    <span class="comment"># *****END OF CORE CODE*****</span></span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br></pre></td></tr></table></figure>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="mAP"><a href="#mAP" class="headerlink" title="mAP"></a>mAP</h3><p>$$<br>    \mathrm{mAP} = \frac{\Sigma_{i=1}^{n} \mathrm{AP}_i}{n}<br>$$</p>
<p>$$<br>    \mathrm{AP}_i = \Sigma_j \mathrm{precision}_j (\mathrm{recall}<em>j - \mathrm{recall}</em>{j-1})<br>$$</p>
<h3 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">ixmin = max(pred_bbox[<span class="number">0</span>], gt_bbox[<span class="number">0</span>])</span><br><span class="line">iymin = max(pred_bbox[<span class="number">1</span>], gt_bbox[<span class="number">1</span>])</span><br><span class="line">ixmax = min(pred_bbox[<span class="number">2</span>], gt_bbox[<span class="number">2</span>])</span><br><span class="line">iymax = min(pred_bbox[<span class="number">3</span>], gt_bbox[<span class="number">3</span>])</span><br><span class="line">iw = np.maximum(ixmax - ixmin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line">ih = np.maximum(iymax - iymin + <span class="number">1.</span>, <span class="number">0.</span>)</span><br><span class="line"></span><br><span class="line">inters = iw * ih</span><br><span class="line"></span><br><span class="line">area_pred = (pred_bbox[<span class="number">2</span>] - pred_bbox[<span class="number">0</span>] + <span class="number">1.</span>) * (pred_bbox[<span class="number">3</span>] - pred_bbox[<span class="number">1</span>] + <span class="number">1.</span>)</span><br><span class="line">area_gt = (gt_bbox[<span class="number">2</span>] - gt_bbox[<span class="number">0</span>] + <span class="number">1.</span>) * (gt_bbox[<span class="number">3</span>] - gt_bbox[<span class="number">1</span>] + <span class="number">1.</span>)</span><br><span class="line">uni = area_pred + area_gt - inters</span><br><span class="line"></span><br><span class="line">iou = inters / uni</span><br></pre></td></tr></table></figure>
<h3 id="nms"><a href="#nms" class="headerlink" title="nms"></a>nms</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">py_cpu_nms</span><span class="params">(dets, thresh)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    nms</span></span><br><span class="line"><span class="string">    :param dets: ndarray [x1,y1,x2,y2,score]</span></span><br><span class="line"><span class="string">    :param thresh: int</span></span><br><span class="line"><span class="string">    :return: list[index]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x1 = dets[:, <span class="number">0</span>]</span><br><span class="line">    y1 = dets[:, <span class="number">1</span>]</span><br><span class="line">    x2 = dets[:, <span class="number">2</span>]</span><br><span class="line">    y2 = dets[:, <span class="number">3</span>]</span><br><span class="line">    order = dets[:, <span class="number">4</span>].argsort()[::<span class="number">-1</span>]</span><br><span class="line">    area = (x2 - x1 + <span class="number">1</span>) * (y2 - y1 + <span class="number">1</span>)</span><br><span class="line">    keep = []</span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</span><br><span class="line">        i = order[<span class="number">0</span>]</span><br><span class="line">        keep.append(i)</span><br><span class="line">        xx1 = np.maximum(x1[i], x1[order[<span class="number">1</span>:]])</span><br><span class="line">        yy1 = np.maximum(y1[i], y1[order[<span class="number">1</span>:]])</span><br><span class="line">        xx2 = np.minimum(x2[i], x2[order[<span class="number">1</span>:]])</span><br><span class="line">        yy2 = np.minimum(y2[i], y2[order[<span class="number">1</span>:]])</span><br><span class="line">        w = np.maximum(<span class="number">0</span>, xx2 - xx1 + <span class="number">1</span>)</span><br><span class="line">        h = np.maximum(<span class="number">0</span>, yy2 - yy1 + <span class="number">1</span>)</span><br><span class="line">        over = (w * h) / (area[i] + area[order[<span class="number">1</span>:]] - w * h)</span><br><span class="line">        index = np.where(over &lt;= thresh)[<span class="number">0</span>]</span><br><span class="line">        order = order[index + <span class="number">1</span>] <span class="comment"># 不包括第0个</span></span><br><span class="line">    <span class="keyword">return</span> keep</span><br><span class="line"><span class="comment"># ————————————————</span></span><br><span class="line"><span class="comment"># 版权声明：本文为CSDN博主「leo_fighting」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。</span></span><br><span class="line"><span class="comment"># 原文链接：https://blog.csdn.net/zhangliaobet/article/details/99699675</span></span><br></pre></td></tr></table></figure>
<h3 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># X, Y: N*D</span></span><br><span class="line">dists = np.sqrt(<span class="number">-2</span> * X @ Y.T + np.sum(np.square(X), axis=<span class="number">1</span>) + np.sum(np.square(Y), axis=<span class="number">1</span>)[:,<span class="keyword">None</span>])</span><br></pre></td></tr></table></figure>
      
    </div>

    

    
    
    

    

    
      
    
    

    
      <div>
        



  



<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>4F5DA2</li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    
    <a href="http://blog.4f5da2.com/2021/03/31/nn-basic/" title="神经网络常用运算&优化器">http://blog.4f5da2.com/2021/03/31/nn-basic/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li>
</ul>

      </div>
    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/01/04/paper-writing/" rel="next" title="论文写作核对清单">
                <i class="fa fa-chevron-left"></i> 论文写作核对清单
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">4F5DA2</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">26</span>
                    <span class="site-state-item-name">日志</span>
                  </a>
                </div>
              

              

              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/0x4f5da2" title="GitHub &rarr; https://github.com/0x4f5da2" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://steamcommunity.com/id/0x4f5da2" title="Steam &rarr; https://steamcommunity.com/id/0x4f5da2" rel="noopener" target="_blank"><i class="fa fa-fw fa-steam"></i>Steam</a>
                </span>
              
            </div>
          

          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                友情链接
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://great-li-xin.github.io" title="https://great-li-xin.github.io" rel="noopener" target="_blank">Great-Li-Xin</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.lightina.cn" title="http://blog.lightina.cn" rel="noopener" target="_blank">jacklightChen</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://blog.handora.me" title="http://blog.handora.me" rel="noopener" target="_blank">Handora</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.jianshu.com/u/27854b505130" title="https://www.jianshu.com/u/27854b505130" rel="noopener" target="_blank">413x</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/oneday_pyf" title="https://blog.csdn.net/oneday_pyf" rel="noopener" target="_blank">OneDay_pyf</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://yisaer.github.io" title="https://yisaer.github.io" rel="noopener" target="_blank">Yisaer</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://blog.csdn.net/m_oman" title="https://blog.csdn.net/m_oman" rel="noopener" target="_blank">CoCoManYY</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://me.csdn.net/yr12Dong" title="https://me.csdn.net/yr12Dong" rel="noopener" target="_blank">12Dong</a>
                  </li>
                
              </ul>
            </div>
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#常用运算及其求导"><span class="nav-number">1.</span> <span class="nav-text">常用运算及其求导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#softmax"><span class="nav-number">1.1.</span> <span class="nav-text">softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#svm-loss"><span class="nav-number">1.2.</span> <span class="nav-text">svm loss</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fully-connected"><span class="nav-number">1.3.</span> <span class="nav-text">fully connected</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#batch-norm"><span class="nav-number">1.4.</span> <span class="nav-text">batch norm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#dropout"><span class="nav-number">1.5.</span> <span class="nav-text">dropout</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#conv"><span class="nav-number">1.6.</span> <span class="nav-text">conv</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#max-pool"><span class="nav-number">1.7.</span> <span class="nav-text">max pool</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#常用优化器"><span class="nav-number">2.</span> <span class="nav-text">常用优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#sgd"><span class="nav-number">2.1.</span> <span class="nav-text">sgd</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sgd-w-momentum"><span class="nav-number">2.2.</span> <span class="nav-text">sgd w/ momentum</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rmsprop"><span class="nav-number">2.3.</span> <span class="nav-text">rmsprop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#adam"><span class="nav-number">2.4.</span> <span class="nav-text">adam</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他"><span class="nav-number">3.</span> <span class="nav-text">其他</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#mAP"><span class="nav-number">3.1.</span> <span class="nav-text">mAP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#IoU"><span class="nav-number">3.2.</span> <span class="nav-text">IoU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#nms"><span class="nav-number">3.3.</span> <span class="nav-text">nms</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#欧式距离"><span class="nav-number">3.4.</span> <span class="nav-text">欧式距离</span></a></li></ol></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">4F5DA2</span>

  

  
</div>


  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v6.7.0</div>




        


  <script>
    var _mtac = {};
    (function() {
      var mta = document.createElement("script");
      mta.src = "https://pingjs.qq.com/h5/stats.js";
      mta.setAttribute("name", "MTAH5");
      mta.setAttribute("sid", "500654405");
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(mta, s);
    })();
  </script>



  <script>
    (function() {
      var hm = document.createElement("script");
      hm.src = "//tajs.qq.com/stats?sId=65819248";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>





        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>




  
  <script src="/js/src/scrollspy.js?v=6.7.0"></script>
<script src="/js/src/post-details.js?v=6.7.0"></script>



  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'WniJVKEoQxAowKO3HKtNs4vX-gzGzoHsz',
    appKey: 'Cl8lmwarMB7thoEPmBOONarz',
    placeholder: 'ヾﾉ≧∀≦)o 来呀！快活呀！~',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false
  });
</script>




  


  





  

  

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
      }
      else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


  

  
  

  
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style>

    
  


  

  

  

  

  

  

  

  

  <div id="particles-js" style="position: fixed; top: 0px; left: 0px; z-index: -1; width: 100%; height: 100%"></div>
  <script src="/js/src/particles.js"></script>
  <script>particlesJS.load('particles-js', '/js/src/particlesjs-config.json')</script>
</body>
</html>
